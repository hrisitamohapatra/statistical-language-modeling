# ğŸ“˜ Statistical Language Modeling using N-grams (Unigram, Bigram, Trigram)

## ğŸ“Œ Overview

This project implements **statistical language models** using **unigrams, bigrams, and trigrams** on a literary English corpus sourced from **Project Gutenberg**.

The model estimates word sequence probabilities using **Laplace (Add-1) smoothing**, computes sentence probabilities, and performs **interactive next-word prediction** based on probabilistic context.

The goal of this project is to understand the **foundations of language modeling** used in early NLP systems and to gain hands-on experience with **probabilistic text modeling**.

---

## ğŸ“š Corpus Details

The corpus is built by combining multiple **public-domain English novels** to ensure linguistic diversity and sufficient vocabulary coverage.

### Books Used
- *Pride and Prejudice* â€“ Jane Austen  
- *Little Women* â€“ Louisa May Alcott  
- *Aliceâ€™s Adventures in Wonderland* â€“ Lewis Carroll  
- *The Adventures of Sherlock Holmes* â€“ Arthur Conan Doyle  

All texts are sourced from **Project Gutenberg**.

---

## âš™ï¸ Preprocessing Steps

The following preprocessing steps are applied to the raw text:

- Conversion to lowercase  
- Removal of punctuation and non-alphabetic characters  
- Normalization of whitespace  
- Tokenization into word sequences  

---

## ğŸ§  Language Models Implemented

### 1ï¸âƒ£ Unigram Model
- Counts individual word frequencies  
- Used for vocabulary statistics and fallback probabilities  

---

### 2ï¸âƒ£ Bigram Model
- Models the conditional probability of a word given the previous word  
- Uses **Add-1 (Laplace) smoothing** to handle unseen word pairs  

**Formula:**

\[
P(w_2 \mid w_1) = \frac{C(w_1, w_2) + 1}{C(w_1) + V}
\]

---

### 3ï¸âƒ£ Trigram Model
- Models the conditional probability of a word given the previous two words  
- Uses **Add-1 smoothing** and **backs off to the bigram model** for unseen contexts  

**Formula:**

\[
P(w_3 \mid w_1, w_2) = \frac{C(w_1, w_2, w_3) + 1}{C(w_1, w_2) + V}
\]

---

## âœ¨ Features

- âœ” Corpus construction from multiple books  
- âœ” Unigram, bigram, and trigram frequency computation  
- âœ” Add-1 smoothing for probability estimation  
- âœ” Sentence probability calculation using the bigram model  
- âœ” Interactive next-word prediction  
- âœ” Backoff strategy for unseen trigram contexts  

---

## ğŸ–¥ï¸ Interactive Next-Word Prediction

The program allows users to input:

- **One word** â†’ predicts the next word using the **bigram model**  
- **Two words** â†’ predicts the next word using the **trigram model**


---

## ğŸ› ï¸ Technologies Used

- Python  
- Regular Expressions (`re`)  
- `collections.Counter`  
- HTTP requests (`requests`)  
- Project Gutenberg corpus  

---

## ğŸ“ˆ Learning Outcomes

Through this project, I learned:

- How statistical language models work internally  
- The data sparsity problem in NLP  
- The role of smoothing in probability estimation  
- Differences between unigram, bigram, and trigram contexts  
- How early language models differ from modern neural models  

---

## ğŸ“Œ Future Improvements

- Perplexity evaluation  
- Text generation using n-gram sampling  
- Comparison with neural language models (LSTM / Transformer)  
- `<UNK>` token handling for out-of-vocabulary words  
- GUI or web-based interface  

---

## ğŸ“œ License

All texts used are from **Project Gutenberg** and are in the public domain.  
This project is intended for **educational and academic purposes**.

